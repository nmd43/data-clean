---
title: "Locality Sensitive Hashing"
author: Rebecca C. Steorts (based upon prior work with Andee Kaplan)
output: pdf_document
date: "2024-09-05"
---

## Agenda

- Locality Sensitive Hashing (LSH)
- Hash functions
- Hashed shingles
- Signatures
- Characteristic Matrix
- Minhash (Jaccard Similarity Approximation)
- Back to LSH


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(RecordLinkage)
library(blink)
library(knitr)
library(textreuse) # text reuse/document similarity
library(tokenizers) # shingles
library(devtools)
library(cora)
library(ggplot2)
# install_github("resteorts/cora")
data(cora) # load the cora data set
data(cora_gold) 
head(cora_gold_update) # contains the id and the unique id
tail(cora_gold_update)
dim(cora_gold_update)
```

## LSH

Locality sensitive hashing (LSH) is a fast method of blocking for record linkage that orginates from the computer science literature. 

\begin{itemize}
\item LSH tries to preserve similarity after dimension reduction.
\begin{itemize}
\item What kind of similarity? $\leftrightarrow$ What kind of dimension reduction?
\end{itemize}
\end{itemize}

## Data set

Consider the cora citation data set. 

1. Shingle all records using a shingle size of 3. Then calculate the Jaccard similarity for all record pairs using the shingled records.  

```{r, cache=TRUE, echo=TRUE}
# get only the columns we want
# number of records
n <- nrow(cora)
# create id column
dat <- data.frame(id = seq_len(n))  
# get columns we want
dat <- cbind(dat, cora[, c("title", "authors", "journal")]) 
shingles <- apply(dat, 1, function(x) {
  # tokenize strings
  tokenize_character_shingles(paste(x[-1], collapse=" "), n = 3)[[1]]
})
# empty holder for similarities
jaccard <- expand.grid(record1 = seq_len(n), 
                       record2 = seq_len(n))

# don't need to compare the same things twice
jaccard <- jaccard[jaccard$record1 < jaccard$record2,]


time <- Sys.time() # for timing comparison
jaccard$similarity <- apply(jaccard, 1, function(pair) {
  # get jaccard similarity for each record pair
  jaccard_similarity(shingles[[pair[1]]], shingles[[pair[2]]]) 
})
# timing
time <- difftime(Sys.time(), time, units = "secs") 
head(jaccard)
```

2. Visually plot the Jaccard similarity. What do you observe?

```{r your-turn2-plot, fig.cap="Jaccard similarity for each pair of records. Light blue indicates the two records are more similar and dark blue indicates less similar."}
# plot the jaccard similarities for each pair of records
ggplot(jaccard) +
  geom_raster(aes(x = record1, y = record2, 
                  fill=similarity)) +
  theme(aspect.ratio = 1) +
  scale_fill_gradient("Jaccard similarity") +
  xlab("Record id") + ylab("Record id")
```

\newpage

## Perform LSH

3. To reduce the overall computational complexity, let's use the lsh approximation. 

There an easy way to do LSH using the built in functions in the `textreuse` package via the functions `minhash_generator` and `lsh` (so we don't have to perform it by hand): 

## Find the number of buckets or bands to use 

```{r show-package-lsh, echo=TRUE, cache=TRUE}
library(numbers) 
m <- 360
bin_probs <- expand.grid(s = c(.25, .75), h = m, b = divisors(m))
#bin_probs
# choose appropriate num of bands and number of random permutations m (tuning parameters)
bin_probs$prob <- apply(bin_probs, 1, function(x) lsh_probability(x[["h"]], x[["b"]], x[["s"]]))
# plot as curves
ggplot(bin_probs) +
  geom_line(aes(x = prob, y = b, colour = factor(s), group = factor(s)), size = 2) +
  geom_point(aes(x = prob, y = b, colour = factor(s)), size = 3) +
  xlab("Probability") +
  scale_color_discrete("s")

# create the minhash function
minhash <- minhash_generator(n = m, seed = 02082018)
b <- 90
```

## Build corpus and perform shingling
```{r show-package-lsh-1, echo=TRUE, cache=TRUE}
head(dat)
# build the corpus using textreuse
docs <- apply(dat, 1, function(x) paste(x[-1], collapse = " ")) # get strings
names(docs) <- dat$id # add id as names in vector
corpus <- TextReuseCorpus(text = docs, # dataset
                          tokenizer = tokenize_character_shingles, n = 3, 
                          simplify = TRUE, # shingles
                          progress = FALSE, # quietly
                          keep_tokens = TRUE, # store shingles
                          minhash_func = minhash) # use minhash
head(minhashes(corpus[[1]]))
length(minhashes(corpus[[1]]))
```

Note that all our records are now represented by 360 randomly selected and hashed shingles. Comparing these shingles are equivalent to finding the Jaccard similarity of all the record pairs. We still have an issue of all the pairwise comparison. 


## Find buckets, candidate records, and Jaccard similarity
Now, we find the buckets, candidates records, and calculate the Jaccard similarity for the candidate records (in the buckets)

```{r show-package-lsh-2, echo=TRUE, cache=TRUE}

# perform lsh to get buckets
buckets <- lsh(corpus, bands = b, progress = FALSE)

# grab candidate pairs
candidates <- lsh_candidates(buckets)

# get Jaccard similarities only for candidates
lsh_jaccard <- lsh_compare(candidates, corpus, 
                           jaccard_similarity, progress = FALSE)
head(buckets)
dim(buckets)
length(unique(buckets))
head(lsh_jaccard)
```

We now plot the Jaccard similarities that are candidate pairs (under LSH)

```{r, lsh-plot,echo=FALSE}
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
```

<!-- Finally, we need to move from candidate pairs of records to blocks.  -->

<!-- ```{r, echo=TRUE} -->
<!-- library(igraph) #graph package -->
<!-- # think of each record as a node -->
<!-- # there is an edge between nodes if they are candidates -->
<!-- g <- make_empty_graph(n, directed = FALSE) # empty graph -->
<!-- g <- add_edges(g, is.vector((candidates[, 1:2]))) # candidate edges -->
<!-- g <- set_vertex_attr(g, "id", value = dat$id) # add id -->

<!-- # get custers, these are the blocks -->
<!-- clust <- components(g, "strong") # get clusters -->
<!-- blocks <- data.frame(id = V(g)$id, # record id -->
<!--                      block = clust$membership) # block number  -->
<!-- head(blocks) -->
<!-- tail(blocks) -->
<!-- dim(blocks) -->
<!-- ``` -->






<!-- ## Evaluation Metrics -->
<!-- ```{r} -->
<!-- unique_id <- cora_gold_update$unique_id -->
<!-- length(unique(unique_id)) -->
<!-- head(blocks) # id refers to the id of the record and block refers to the block number -->
<!-- head(cora_gold) # these are record pairs that match one another -->
<!-- head(unique_id) -->
<!-- table(unique_id) -->
<!-- ``` -->









<!-- \newpage -->

<!-- ```{r} -->
<!-- unique_id <- cora_gold_update$unique_id -->
<!-- reduction.ratio <- function(block.labels) { -->
<!--   n_all_comp = choose(length(block.labels), 2) -->
<!--   n_block_comp = sum(choose(table(block.labels), 2)) -->

<!--   (n_all_comp - n_block_comp) / n_all_comp -->
<!-- } -->
<!-- reduction.ratio(blocks$block) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- precision <- function(block.labels, IDs) { -->
<!--   ct = xtabs(~block.labels+IDs) -->

<!--   # Number of true positives -->
<!--   TP = sum(choose(ct, 2)) -->

<!--   # Number of positives = TP + FP -->
<!--   P = sum(choose(rowSums(ct), 2))  -->

<!--   return(TP/P) -->
<!-- } -->

<!-- recall <- function(block.labels, IDs) { -->
<!--   ct = xtabs(~IDs+block.labels) -->

<!--   # Number of true positives -->
<!--   TP = sum(choose(ct, 2)) -->

<!--   # Number of true links = TP + FN -->
<!--   TL = sum(choose(rowSums(ct), 2)) -->

<!--   return(TP/TL) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- precision(blocks$block, unique_id) -->
<!-- recall(blocks$block, unique_id) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(clevr, exchanger) -->
<!-- #true_membership <- true_memberships[[cora]] -->
<!-- #true_pairs <- membership_to_pairs(true_membership) -->
<!-- #record_ids <- seq_along(true_membership) -->
<!-- ``` -->

